{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Data Wrangling Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are going to extract data from different sources related with the Twitter account `@dog_rates`.\n",
    "\n",
    "Basicly we will work with four data sources:\n",
    "\n",
    "* **'twitter-archive-enhanced.csv':**  A csv file with 2356 tweets of this account. Each one with a picture of a dog. They use to mark this dogs, usually with marks greater than 10 over 10: 11/10, 13/10, etc. This file is provided by Udacity for making the project.\n",
    "\n",
    "* **'image-predictions.tsv':** A tsv file with the results obtained of applying a predictive method over the pictures of the tweets. This file was obtained in a project in another nanodegreee and it is provided by Udacity also. Every image in the WeRateDogs Twitter archive was run through a neural network that can classify breeds of dogs. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "\n",
    "* **Additional information obtained with tweetpy:** Once obtained the Twitter credentials, we are going to use the tweetpy API to get more additional data. To do this we will connect to the Twitter platform and using tweetpy we will download the tweet status for each tweet in `twitter-archive-enhanced.csv`. Then we will save these results in a file callde `twitter_archive.json` using the json library. Finally, we will read this file and we will extract some more data to another dataframe using json again.\n",
    "\n",
    "* **Information abour the replies of each tweet:** Finally we would like to extract the data corresponding to the replies of each tweet. We have tried some different methods:\n",
    "\n",
    "    * In some places it is recommended to use tweepy to make a query of all the tweets referenced to @rate_dogs, and search which of them are a reply to the status of the tweet. Translated to code, something like this:\n",
    "    \n",
    "            consumer_key = 'XXXXXX'\n",
    "            consumer_secret = 'XXXXXX'\n",
    "            access_token = 'XXXXXX'\n",
    "            access_secret = 'XXXXXX'\n",
    "\n",
    "            auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "            twapi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "            replies=[]\n",
    "\n",
    "            for tweet in tweepy.Cursor(twapi.search,q='to:'+name, since_id=892420643555336193, result_type='recent',timeout=999999).items(1000)\n",
    "                if hasattr(tweet, 'in_reply_to_status_id_str'):\n",
    "                    if (tweet.in_reply_to_status_id_str==tweet_id):\n",
    "                        replies.append(tweet)\n",
    "            \n",
    "      but it has a lot of limitations and I didn't like it too much.\n",
    "\n",
    "    * In other places it is recommended to use the urllib3 library to request pages. Then, you can use BeautifulSoup to interpret the result and scrapp the information that you need:\n",
    "    \n",
    "             http = urllib3.PoolManager()\n",
    "             url = \"https://twitter.com/dog_rates/status/892420643555336193\"\n",
    "             r = http.request('GET', url)\n",
    "             soup = BeautifulSoup(r.data)\n",
    "             tweets = soup.find_all('li','js-stream-item')\n",
    "             for tweet in tweets:\n",
    "\n",
    "             full_name = tweet.find(\"span\", \"FullNameGroup\").find(\"strong\", \"fullname\").contents[0]  \n",
    "        \n",
    "      But, in this case, you need to make scroll down on the page to see all the replies. Even so, when there are too many replies, the page cut the list and ask you in a link if you want to see more. You had to do this as many times as you need until you reach the end of the list. Apart from that, sometimes there are replies to the replies, and the page has another link to select to see them. I mean that with a single request you can't see all the replies if these are a lot.\n",
    "      Maybe yo can do that using additional requests with POST or some other commands and sending the correct instruction to click in all the necessary links. But I felt like it was too much complicated.\n",
    "       \n",
    "    * Finally, I tried another method to do scrapping. I used the `selenium` library. It permits you to use a local browser to open the pages. You can navigate using the program on these pages and select and click any element of the page. Once you have deployed completely the page you can get it to a beautifulsoup object and interpret it. And using a local browser can be viewed as a disadvantage, but I felt more comfortable with this method and it is what I have used.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#to interact with the local system\n",
    "import os\n",
    "import sys\n",
    "#to work with regular expressions\n",
    "import re\n",
    "#imports to user timers and make conversions of time formats\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import time\n",
    "#to make logs and track those processes that take a long time\n",
    "import logging\n",
    "#to get and interpret information of the web\n",
    "import requests\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first step will be to define a function to connect to Twitter using the API `tweepy`. We will use this function in other cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_twitter():\n",
    "    '''\n",
    "    It connects to Twitter API.\n",
    "    \n",
    "    Returns:\n",
    "        twapi: tweepy.api object to interact with the page.\n",
    "    '''\n",
    "    \n",
    "    #It reads the keys to connect to Twitter API from a local file.\n",
    "    #These keys are hidden to comply with Twitter's API terms and conditions\n",
    "    with open('API keys.txt', mode = 'r') as file:\n",
    "        keys = file.readlines()\n",
    "        \n",
    "    keys = [x.strip() for x in keys] \n",
    "    \n",
    "    consumer_key = keys[0].split(\":\")[1]\n",
    "    consumer_secret = keys[1].split(\":\")[1]\n",
    "    access_token = keys[2].split(\":\")[1]\n",
    "    access_secret = keys[3].split(\":\")[1]\n",
    "    \n",
    "    #It authenticates in tweepy with the previous credentials.\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    twapi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    return twapi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the folder where to save the necesary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It creates a folder called resources if it does not exists\n",
    "folder_name = 'resources'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data from `twitter-archive-enhanced.csv` file supplied by Udacity. This dataframe has the following columns:\n",
    "\n",
    "    - **tweet_id:** The integer representation of the unique identifier for this Tweet. \n",
    "    - **in_reply_to_status_id:** If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet’s ID.\n",
    "    - **in_reply_to_user_id:**  If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet’s author ID. This will not necessarily always be the user directly mentioned in the Tweet.\n",
    "    - **timestamp:** date and time of the tweet.\n",
    "    - **source:** Utility used to post the Tweet, as an HTML-formatted string. Tweets from the Twitter website have a source value of web.\n",
    "    - **text:** The actual UTF-8 text of the status update. \n",
    "    - **retweeted_status_id:** If the represented Tweet is a retweet, this field will contain the integer representation of the original Tweet’s ID. If it is a retweet of a retweet it containg the original message id.\n",
    "    - **retweeted_status_user_id:**  If the represented Tweet is a retweet, this field will contain the integer representation of the original Tweet’s author ID. This will not necessarily always be the user directly mentioned in the Tweet.\n",
    "    - **retweeted_status_timestamp:** If the represented Tweet is a retweet, the timestampo of the original tweet.\n",
    "    - **expanded_urls:** url of the tweet.\n",
    "    - **rating_numerator:** numerator of the rating assigned according to the text of the tweet.\n",
    "    - **rating_denominator:** denominator of the rating assigned according to the text of the tweet.\n",
    "    - **name:** name of the dog according to the text of the tweet.\n",
    "    - **doggo:** type of the dog acording to the text and to the clasification used in the page.\n",
    "    - **floofer:** type of the dog acording to the text and to the clasification used in the page.\n",
    "    - **pupper:** type of the dog acording to the text and to the clasification used in the page.\n",
    "    - **puppo:** type of the dog acording to the text and to the clasification used in the page. \n",
    "    \n",
    "    \n",
    "![alt text](dogtionary-combined.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load the file twitter-archive-enhanced.csv into a dataframe\n",
    "df_twitter_archive_enhanced = pd.read_csv(os.path.join(folder_name, 'twitter-archive-enhanced.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data from the `image-predictions.tsv` file. This file was provided by Udacity in a especified url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We donwload the image-predictions.tsv file from the expecified url.\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "with open(os.path.join(folder_name, url.split('/')[-1]), mode = 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We crate a new dataframe called `df_image_predictions`. This dataframe has the following columns:\n",
    "\n",
    "    - **tweet_id:** tweet_id is the last part of the tweet URL after \"status/\": `https://twitter.com/dog_rates/status/889531135344209921`\n",
    "    \n",
    "    - **jpg_url:** url of the image of the tweet. It can be downloaded.\n",
    "    - **img_num:** the image with the most confident prediction.\n",
    "    - **p1:** is the algorithm's #1 prediction for the image in the tweet.\n",
    "    - **p1_conf:** is how confident the algorithm is in its #1 prediction.\n",
    "    - **p1_dog:** is whether or not the #1 prediction is a breed of dog.\n",
    "    - **p2:** is the algorithm's second most likely prediction.\n",
    "    - **p2_conf:** is how confident the algorithm is in its #2 prediction.\n",
    "    - **p2_dog:** is whether or not the #2 prediction is a breed of dog.\n",
    "    - **p3:** is the algorithm's third most likely prediction.\n",
    "    - **p3_conf:** is how confident the algorithm is in its #3 prediction.\n",
    "    - **p3_dog:** is whether or not the #3 prediction is a breed of dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the file image-predictions.tsv into the dataframe df_image_predictions\n",
    "df_image_predictions = pd.read_csv(os.path.join(folder_name, 'image-predictions.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load additional data from the pages of the tweets with the API tweepy. At the same time, we can see which pages still exist and which are not available in this moment. First, we download the content of the page of each tweet and we store it in a file calle `twitter_archive.json`, in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "=================================================\n",
    "REMOVE THIS TO EXECUTE IT. IT CAN TAKES MORE THAN AN HOUR.\n",
    "=================================================\n",
    "#we connect to the Twitter API using tweepy \n",
    "twapi = connect_twitter()\n",
    "\n",
    "start = timer()\n",
    "#It configures the file 'tweepy_api.log' as a log to track the evolution\n",
    "logging.basicConfig(filename='tweepy_api.log',level=logging.DEBUG)\n",
    "\n",
    "#It initializes a list of Id's with all the tweets.\n",
    "list_ids = df_twitter_archive_enhanced.tweet_id\n",
    "\n",
    "total_count = 0\n",
    "error_count = 0\n",
    "\n",
    "#It initializes the log file\n",
    "open('tweepy_api.log', 'w').close()\n",
    "\n",
    "#It initializes the files used to save the results.\n",
    "#We have created two files: one estructured in lines and other indented for a more\n",
    "#friendly check.\n",
    "open(os.path.join(folder_name, 'twitter_archive_indent.json'), 'w').close()\n",
    "open(os.path.join(folder_name, 'twitter_archive.json'), 'w').close()\n",
    "\n",
    "for tweet_id in list_ids:\n",
    "    total_count += 1\n",
    "    logging.debug('%s: Trying tweet for ID %s', total_count, tweet_id)\n",
    "    try:\n",
    "        #download the content of a tweet for a tweet_id given\n",
    "        tweet = twapi.get_status(tweet_id, tweet_mode='extended')\n",
    "        #store the content of the tweet using json in the file tweet_json_indent.txt, indent=2 spaces.\n",
    "        with open(os.path.join(folder_name, 'twitter_archive_indent.json'), 'a', encoding='utf8', newline='\\n') as out_file:\n",
    "            json.dump(tweet._json, out_file, indent=2, ensure_ascii=False)\n",
    "            out_file.write('\\n')\n",
    "        #store the content of the tweet using json in the file tweet_json.txt, in a sigle line.\n",
    "        with open(os.path.join(folder_name, 'twitter_archive.json'), 'a', encoding='utf8', newline='\\n') as out_file:\n",
    "            json.dump(tweet._json, out_file, ensure_ascii=False)\n",
    "            out_file.write('\\n')\n",
    "            \n",
    "        #separate each tweet in the log file.    \n",
    "        logging.debug('============================================================================')\n",
    "        logging.debug('============================================================================')\n",
    "    except tweepy.TweepError as te:\n",
    "        #if we cannot download the tweet, we reflect this in the log and we increment the error count.\n",
    "        logging.warning('%s: FAILED to get tweet ID %s: %s', total_count, tweet_id, str(te))\n",
    "        error_count += 1\n",
    "                    \n",
    "    end = timer()\n",
    "    #separate each tweet in the log file. \n",
    "    logging.debug('TOTAL: %s: TIME %s%s', total_count, end-start,'===========================================')\n",
    "    logging.debug('TOTAL: %s. ERRORS: %s%s', total_count, error_count,'===========================================')\n",
    "    logging.debug('============================================================================')\n",
    "    logging.debug('============================================================================')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total count of tweets in this new file and number of errors. The errors are pages that existed when Udacity extracted the file `twitter-archive-enhanced.csv`, but now are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2356, 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count, error_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the data stored in the file `twitter_archive.json` in the previous step to create a new dataframe called `df_tweepy_extractions`. This dataframe will have the following columns:\n",
    "\n",
    "    - **tweet_id:** The integer representation of the unique identifier for this Tweet.\n",
    "    - **entities_name:** Users who are labelled under the picture of the tweet.\n",
    "    - **entities_screen_name:** Screen name of the users who are labelled under the picture of the tweet.\n",
    "    - **entities_type:** The type of the entity. In this case is always 'user'\n",
    "    - **entities_user_id:** ID of the users who are labelled under the picture of the tweet.\n",
    "    - **favorite_count:**  Indicates approximately how many times this Tweet has been liked by Twitter users. \n",
    "    - **favorites_count_retweet:** This field only surfaces when the Tweet is a retweet. Indicates approximately how many times the original Tweet has been liked by Twitter users. \n",
    "    - **mentions_name:** Display name of the referenced user in the text of the tweet.\n",
    "    - **mentions_screen_name:** Screen name of the referenced user in the text of the tweet.\n",
    "    - **mentions_user_id:** ID of the mentioned user, as an integer in the text of the tweet.\n",
    "    - **quoted_status_id:** This field only surfaces when the Tweet is a quote Tweet. This field contains the integer value Tweet ID of the quoted Tweet. \n",
    "    - **quoted_user_id:** ID of the user quoted.\n",
    "    - **quoted_status_id_rwetweet:** This field only surfaces when the Tweet is a retweet and the original Tweet is a is a quote Tweet. This field contains the integer value Tweet ID of the quoted Tweet.\n",
    "    - **retweet_count:** Number of times this Tweet has been retweeted.\n",
    "    - **retweet_count_retweet:** This field only surfaces when the Tweet is a retweet. Indicates approximately how many times the original Tweet has been retweeted.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we first read the file and load the lines in a list called content\n",
    "with open(os.path.join(folder_name, 'twitter_archive.json'), 'r', encoding='utf8') as input_file:\n",
    "    content = input_file.readlines()\n",
    "content = [x.strip() for x in content] \n",
    "\n",
    "#initialize the result dataframe df_tweepy_extractions\n",
    "df_tweepy_extractions = pd.DataFrame()\n",
    "tweet_status = {}\n",
    "\n",
    "#read recursively each line in the list content\n",
    "for line in content:\n",
    "    #initialize the outcomes\n",
    "    entities_name = ''\n",
    "    entities_screen_name  = ''\n",
    "    entities_type = ''\n",
    "    entities_user_id = ''\n",
    "    mentions_user_id = ''\n",
    "    mentions_name = ''\n",
    "    mentions_screen_name = ''\n",
    "    favorites_count_retweet = ''\n",
    "    retweet_count_retweet = ''\n",
    "    quoted_status_id_rwetweet = ''\n",
    "    quoted_status_id_str = ''\n",
    "    \n",
    "    #read each string with json.loads to interpretarte it\n",
    "    tweet_status = json.loads(line)\n",
    "    try:\n",
    "        #if the object media exists innside entities.\n",
    "        if 'media' in tweet_status['entities']:\n",
    "            #In this case we are going to see the user or users tagged below the picture\n",
    "            if 'all' in tweet_status['entities']['media'][0]['features']:\n",
    "                #a list with the names of the users tagged\n",
    "                entities_name = [name['name'] for name in tweet_status['entities']['media'][0]['features']['all']['tags']]\n",
    "                #the screen names eje:@bla_bla_bla\n",
    "                entities_screen_name = [screen_name['screen_name'] for screen_name in tweet_status['entities']['media'][0]['features']['all']['tags']]\n",
    "                #types: user\n",
    "                entities_type = [type_['type'] for type_ in tweet_status['entities']['media'][0]['features']['all']['tags']]\n",
    "                #a list with the users id\n",
    "                entities_user_id = [user_id['user_id'] for user_id in tweet_status['entities']['media'][0]['features']['all']['tags']]\n",
    "        #if the object user_mentions exists innside entities. We can search the users named inside te text part.\n",
    "        if 'user_mentions' in tweet_status['entities']:\n",
    "            if len(tweet_status['entities']['user_mentions']) > 0:\n",
    "                #the id of the user mentioned\n",
    "                mentions_user_id = [user_id['id_str'] for user_id in tweet_status['entities']['user_mentions']]\n",
    "                #the name of the user mentioned\n",
    "                mentions_name = [name['name'] for name in tweet_status['entities']['user_mentions']]\n",
    "                #the screen name of the user mentioned\n",
    "                mentions_screen_name = [screen_name['name'] for screen_name in tweet_status['entities']['user_mentions']]\n",
    "        #if the tweet is a retweet\n",
    "        if 'retweeted_status' in tweet_status:\n",
    "            if 'favorite_count' in tweet_status['retweeted_status']:\n",
    "                #number of favorites in the original tweet.\n",
    "                favorites_count_retweet = tweet_status['retweeted_status']['favorite_count']\n",
    "                #number of retweets in the original tweet.\n",
    "                retweet_count_retweet = tweet_status['retweeted_status']['retweet_count']\n",
    "            if 'quoted_status_id_str' in tweet_status['retweeted_status']:\n",
    "                #if the tweet is a retweet of a previously quoted tweet. The tweet id of the original quoted tweet.\n",
    "                quoted_status_id_rwetweet = tweet_status['retweeted_status']['quoted_status_id_str']\n",
    "        #if the tweet is a quoted ot other tweet.\n",
    "        if 'quoted_status_id_str' in tweet_status:\n",
    "            #the id of the quoted tweet.\n",
    "            quoted_status_id_str = tweet_status['quoted_status_id_str']\n",
    "            if 'quoted_status' in tweet_status:\n",
    "                #id of the user quoted.\n",
    "                quoted_user_id_str = tweet_status['quoted_status']['user']['id_str']\n",
    "    \n",
    "    except Exception as e:\n",
    "        #register in the log any exception that it can occurs.\n",
    "        logging.warning(e)\n",
    "    #save the results in the dataframe df_tweepy_extractions\n",
    "    df_tweepy_extractions = df_tweepy_extractions.append({'tweet_id': tweet_status['id_str'],\n",
    "                                                          'retweet_count': tweet_status['retweet_count'],\n",
    "                                                          'favorite_count': tweet_status['favorite_count'],\n",
    "                                                          'favorites_count_retweet': favorites_count_retweet,\n",
    "                                                          'retweet_count_retweet': retweet_count_retweet,\n",
    "                                                          'entities_name': entities_name,\n",
    "                                                          'entities_screen_name': entities_screen_name,\n",
    "                                                          'entities_type': entities_type,\n",
    "                                                          'entities_user_id': entities_user_id,\n",
    "                                                          'mentions_user_id': mentions_user_id,\n",
    "                                                          'mentions_name': mentions_name,\n",
    "                                                          'mentions_screen_name': mentions_screen_name,\n",
    "                                                          'quoted_status_id': quoted_status_id_str,\n",
    "                                                          'quoted_user_id': quoted_user_id_str,\n",
    "                                                          'quoted_status_id_rwetweet': quoted_status_id_rwetweet\n",
    "                                                          },ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapping Replies Using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally we are going to sacrap more information about each tweet using the library selenium. We are interested into obtain information about all the replies for each tweet. We will get a final dataframe called `df_scrapped_replies` with the following columns:\n",
    "\n",
    "    - **conversation:** Id of the replied tweet.\n",
    "    - **favs:** Number of favorites for this replying tweet.\n",
    "    - **full_name:** name of the user who has replied.\n",
    "    - **image:** If there is an image in the reply, it especifies the url.\n",
    "    - **language:** When present, indicates a BCP 47 language identifier corresponding to the machine-detected language of the Tweet text.\n",
    "    - **references:** Other users ID that are referenced in the text of the reply, if they exist.\n",
    "    - **replies:** Number of replies to this reply.\n",
    "    - **reply_id:** tweet ID for this reply.\n",
    "    - **retweets:** Number of retweets of this reply.\n",
    "    - **text:** Text include in the reply.\n",
    "    - **timestamp:** Date_time of the reply.\n",
    "    - **user_id:** Id of the user who has replied.\n",
    "    - **user_name:** Name of the user who has replied (@XXXXX).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This fuction download a status page with all its replies into a driver object. We have user the Firefox driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(driver, user_name, conversation_id):\n",
    "\n",
    "    '''\n",
    "    This fuction download a status page with all its replies into a driver object. We have user the Firefox driver.\n",
    "    \n",
    "    Args:\n",
    "        driver: selenium.webdriver object. Used to get the page, to move on it and make actions.\n",
    "        (str) user_name: user name of the twitter profile. In this project: dog_rates.\n",
    "        (str) conversation_id: status or tweet id for which we want to extract the data.\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "        driver: selenium.webdriver object. The same object, but with all the replies to the tweet opened\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Initialize a new file for the log.\n",
    "    logging.basicConfig(filename='scrapping_replies.log',level=logging.DEBUG)\n",
    "    #url of the page that we want to download\n",
    "    url = \"https://twitter.com/\" + user_name + \"/status/\" + conversation_id\n",
    "    #time to wait after each scroll\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    # tells WebDriver to poll the DOM for a certain amount of time when trying \n",
    "    #to find any element (or elements) not immediately available.\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    driver.get(url)\n",
    "    #we use the length of the page to know if we have downloaded the complete page.\n",
    "    last_length = len(driver.page_source)\n",
    "    \n",
    "    count = 0\n",
    "    while True:\n",
    "        #scroolls down to the end of the page\n",
    "        driver.find_element_by_tag_name(\"body\").send_keys(Keys.END)\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        #the page is actualized with more replies. We get the new length\n",
    "        new_length = len(driver.page_source)\n",
    "        count += 1\n",
    "        if count == 6:\n",
    "            count = 0\n",
    "        #Even when we have reached the end we wait 4*two weconds, just in case the page is not complete downloaded\n",
    "        if (new_length == last_length) & (count > 4):\n",
    "            count = 0\n",
    "            #When the page is completely downloaded and there are not more replies to show, \n",
    "            #we search for the le link 'show nore replies' and we click it.\n",
    "            try:             \n",
    "                button_more = driver.find_element_by_css_selector('.ThreadedConversation-showMoreThreadsButton.u-textUserColor')\n",
    "                button_more.click()\n",
    "            except (NoSuchElementException, AttributeError) as e:\n",
    "                logging.warning(e)\n",
    "                break\n",
    "        #se set the old length equal to the new length to start again the scroll down process.\n",
    "        last_length = new_length\n",
    "    #When there are not more replies neither any 'Show nore replies' link.\n",
    "    #We click in all the intermediate links 'x replies more'. Sometimes people replies to the replies and this other replies\n",
    "    #are not always showed at first.\n",
    "    try:\n",
    "        links_replies = driver.find_elements_by_css_selector('.ThreadedConversation-moreRepliesLink')\n",
    "        for link in links_replies:\n",
    "            link.click()\n",
    "    except (NoSuchElementException, AttributeError) as e:\n",
    "        logging.warning(e)\n",
    "    #return the driver object with the complete page.\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function searchs for information inside the page stored in driver and it saves the content in the dataframe `df_scrapping_replies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_page(driver, conversation_id):\n",
    "    \n",
    "    '''\n",
    "    It searchs for information inside the page stored in driver and it saves the content in the dataframe df_scrapping_replies.\n",
    "    \n",
    "    Args:\n",
    "        driver: selenium.webdriver object. Used to get the page, to move on it and make actions.\n",
    "        (str) conversation_id: status or tweet id for which we want to extract the data.\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "        df_scrapping_replies: pandas.dataframe object with all the data gathered.\n",
    "    '''\n",
    "    \n",
    "    #it configures the file for the log.\n",
    "    logging.basicConfig(filename='scrapping_replies.log',level=logging.DEBUG)\n",
    "    #it initializes the dataframe df_scrapping_replies\n",
    "    df_scrapping_replies = pd.DataFrame()\n",
    "    #call BeautifulSoup with the driver page to be decoded\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    #load the content in a list of replies\n",
    "    tweets = soup.find_all('li','js-stream-item')\n",
    "\n",
    "    for tweet in tweets:\n",
    "        #analyze each reply one by one\n",
    "        try:\n",
    "            #the status id of the reply\n",
    "            reply_id = tweet.get('data-item-id')\n",
    "            full_name = \"\"\n",
    "            #we get only the text part of the name. There is other parts like emojis.\n",
    "            full_names = tweet.find(\"span\", \"FullNameGroup\").find(\"strong\", \"fullname\").contents\n",
    "            for name in full_names:\n",
    "                if isinstance(name,  str):\n",
    "                    full_name = full_name + name\n",
    "                    \n",
    "            #the name with @\n",
    "            user_name = tweet.find(\"span\", \"username\").find(\"b\").contents[0].strip()\n",
    "            #the id of the user.\n",
    "            user_id = tweet.find(\"div\",class_=re.compile(\"^tweet js-stream-tweet\")).get('data-user-id')\n",
    "            logging.debug(user_id)\n",
    "            #The number of replies to this reply, number ot retweets and faver of this reply.\n",
    "            replies = tweet.find(\"span\", id=re.compile(\"^profile-tweet-action-reply-count\")).contents[0]\n",
    "            retweets = tweet.find(\"span\", id=re.compile(\"^profile-tweet-action-retweet-count\")).contents[0]\n",
    "            favs = tweet.find(\"span\", id=re.compile(\"^profile-tweet-action-favorite-count\")).contents[0]\n",
    "            #The language of the message if it is configured.\n",
    "            language = tweet.find(\"p\", \"TweetTextSize js-tweet-text tweet-text\").get('lang')\n",
    "            #In the text part we only get the str part. We discard emojis and other things.\n",
    "            texts = tweet.find(\"p\", \"TweetTextSize js-tweet-text tweet-text\").contents\n",
    "            text = \"\"\n",
    "            ref = \"\"\n",
    "            image = \"\"\n",
    "            for subtext in texts:\n",
    "                if isinstance(subtext,  str):\n",
    "                    text = text + subtext\n",
    "            ref_aux = tweet.find_all(\"a\", \"pretty-link js-user-profile-link\")\n",
    "            #the other users id that are referred in the text of the reply\n",
    "            for subref in ref_aux:\n",
    "                subrefs = subref.get('data-user-id')\n",
    "                ref.append(subrefs)\n",
    "            #We try to find if there is some image attached. If it is so, we save the url of the picture.\n",
    "            try:\n",
    "                image = tweet.find(\"div\", \"AdaptiveMedia-photoContainer js-adaptive-photo\").get('data-image-url') \n",
    "            except AttributeError as e:\n",
    "                logging.warning(e)\n",
    "            #we also get the date_time of the reply\n",
    "            timestamp = tweet.find(\"small\", \"time\").find(\"span\", \"_timestamp js-short-timestamp\").get('data-time')\n",
    "            timestamp = str(datetime.fromtimestamp(int(timestamp)))\n",
    "            \n",
    "            #se save all in the dataframe df_scrapping_replies \n",
    "            df_scrapping_replies = df_scrapping_replies.append({'timestamp': timestamp,\n",
    "                                                                'conversation': conversation_id,\n",
    "                                                                'reply_id': reply_id,\n",
    "                                                                'full_name': full_name,\n",
    "                                                                'user_name': '@' + user_name,\n",
    "                                                                'user_id': user_id,\n",
    "                                                                'image': image,\n",
    "                                                                'replies': int(replies.split()[0].replace('.','')),\n",
    "                                                                'retweets': int(retweets.split()[0].replace('.','')),\n",
    "                                                                'favs': int(favs.split()[0].replace('.','')),\n",
    "                                                                'text': text,\n",
    "                                                                'language': language,\n",
    "                                                                'references': ref\n",
    "                                                                },ignore_index=True)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logging.warning(e)\n",
    "    \n",
    "    #It reurns df_scrapping_replies \n",
    "    return df_scrapping_replies\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We call the neccessary functions to make the scrap. As it takes a long time, each time that we search the replies of a tweet we save them in the csv file. Instead of saving the complete file at the end. If there were any problem we would have save the results until this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-a10d49617378>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-a10d49617378>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    =================================================\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "=================================================\n",
    "REMOVE THIS TO EXECUTE IT. IT CAN TAKES SEVERAL HOURS TO FINISH\n",
    "=================================================\n",
    "start = timer()\n",
    "#initialize the log file\n",
    "open('scrapping_replies.log', 'w').close()\n",
    "logging.basicConfig(filename='scrapping_replies.log',level=logging.DEBUG)\n",
    "\n",
    "logging.debug(\"********************** CONNECTING DRIVER TO PAGE **********************\")\n",
    "#initialize the driver.\n",
    "twapi = connect_twitter()\n",
    "driver = webdriver.Firefox()\n",
    "logging.debug(\"********************** CONNECTION DONE!!!! **********************\")\n",
    "user_name = 'dog_rates'\n",
    "i = 1\n",
    "try:\n",
    "    #search for each tweet id in df_tweepy_extractions (the tweets that we know that are available in this moment)\n",
    "    for tweet_id in df_tweepy_extractions['tweet_id']:\n",
    "        #call the download_page function to download the complete page\n",
    "        logging.debug(\"********************** START DOWNLOAD: \" + tweet_id + \"(\" + str(i) + \")\" + ' **********************')\n",
    "        driver = download_page(driver, user_name, tweet_id)\n",
    "        logging.debug(\"********************** END DOWNLOAD: \" + tweet_id + \"(\" + str(i) + \")\" + ' **********************')\n",
    "        #call the analize_page function to extract a dataframe with the results\n",
    "        logging.debug(\"********************** START ANALYSIS: \" + tweet_id + \"(\" + str(i) + \")\" + ' **********************')\n",
    "        df_scrapping_replies = analize_page(driver, tweet_id)\n",
    "        #The first time that we save the resutls in scrapped_replies.csv we open the file in write mode and we write a header.\n",
    "        #The next times we open the file in append mode and we don't write the header.\n",
    "        if i == 1:\n",
    "            df_scrapping_replies.to_csv(os.path.join(folder_name, 'scrapped_replies.csv'), mode='w', encoding='utf-8', index=False)\n",
    "        else:\n",
    "            df_scrapping_replies.to_csv(os.path.join(folder_name, 'scrapped_replies.csv'), mode='a', encoding='utf-8', index=False, header = False)\n",
    "        end = timer()\n",
    "        #each time we save a result. we register in the log file the number of tweet_id analyzed and the time consumed.\n",
    "        logging.debug(\"********************** END ANALYSIS: \" + tweet_id + \"(\" + str(i) + \")\" + str(start-end) + ' **********************')\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    logging.warning(e)\n",
    "finally:\n",
    "    #close and disconnect the driver.\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We load the data saved in `scrapped_replies.csv` in the steps before to the dataframe `df_scrapped_replies`. We can see that there are 23 tweets with no replies. We have 2310 rows in df_scrapped_replies and 2333 in df_tweepy_extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2310"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scrapped_replies = pd.read_csv(os.path.join(folder_name, 'scrapped_replies.csv'))\n",
    "df_scrapped_replies.conversation.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2333, 14)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweepy_extractions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_scrapped_replies.conversation = df_scrapped_replies.conversation.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the list of tweets without any replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941    673350198937153538\n",
       "2054    670833812859932673\n",
       "2063    670803562457407488\n",
       "2184    668627278264475648\n",
       "2190    668567822092664832\n",
       "2193    668537837512433665\n",
       "2198    668480044826800133\n",
       "2201    668291999406125056\n",
       "2241    667538891197542400\n",
       "2245    667517642048163840\n",
       "2257    667393430834667520\n",
       "2264    667177989038297088\n",
       "2286    666804364988780544\n",
       "2289    666776908487630848\n",
       "2292    666691418707132416\n",
       "2302    666418789513326592\n",
       "2304    666407126856765440\n",
       "2316    666102155909144576\n",
       "2318    666094000022159362\n",
       "2320    666073100786774016\n",
       "2325    666055525042405380\n",
       "2327    666050758794694657\n",
       "2331    666029285002620928\n",
       "Name: tweet_id, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweepy_extractions[~df_tweepy_extractions.tweet_id.isin(df_scrapped_replies.conversation)].tweet_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `df_twitter_archive_enhanced`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>retweeted_status_timestamp</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>672245253877968896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-03 02:45:32 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Meet Snickers. He's adorable. Also comes in t-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/672245253...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Snickers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>667937095915278337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-11-21 05:26:27 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This dog resembles a baked potato. Bed looks u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/667937095...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>785533386513321988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-10-10 17:32:08 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Dallas. Her tongue is ridiculous. 11/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/785533386...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>821149554670182400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-17 00:18:04 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Luca. He got caught howling. H*ckin em...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/821149554...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Luca</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>675146535592706048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-11 02:54:12 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Coops. He's yelling at the carpet. Not...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/675146535...</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>Coops</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "2011  672245253877968896                    NaN                  NaN   \n",
       "2239  667937095915278337                    NaN                  NaN   \n",
       "706   785533386513321988                    NaN                  NaN   \n",
       "428   821149554670182400                    NaN                  NaN   \n",
       "1872  675146535592706048                    NaN                  NaN   \n",
       "\n",
       "                      timestamp  \\\n",
       "2011  2015-12-03 02:45:32 +0000   \n",
       "2239  2015-11-21 05:26:27 +0000   \n",
       "706   2016-10-10 17:32:08 +0000   \n",
       "428   2017-01-17 00:18:04 +0000   \n",
       "1872  2015-12-11 02:54:12 +0000   \n",
       "\n",
       "                                                 source  \\\n",
       "2011  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2239  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "706   <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "428   <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1872  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                   text  retweeted_status_id  \\\n",
       "2011  Meet Snickers. He's adorable. Also comes in t-...                  NaN   \n",
       "2239  This dog resembles a baked potato. Bed looks u...                  NaN   \n",
       "706   This is Dallas. Her tongue is ridiculous. 11/1...                  NaN   \n",
       "428   This is Luca. He got caught howling. H*ckin em...                  NaN   \n",
       "1872  This is Coops. He's yelling at the carpet. Not...                  NaN   \n",
       "\n",
       "      retweeted_status_user_id retweeted_status_timestamp  \\\n",
       "2011                       NaN                        NaN   \n",
       "2239                       NaN                        NaN   \n",
       "706                        NaN                        NaN   \n",
       "428                        NaN                        NaN   \n",
       "1872                       NaN                        NaN   \n",
       "\n",
       "                                          expanded_urls  rating_numerator  \\\n",
       "2011  https://twitter.com/dog_rates/status/672245253...                12   \n",
       "2239  https://twitter.com/dog_rates/status/667937095...                 3   \n",
       "706   https://twitter.com/dog_rates/status/785533386...                11   \n",
       "428   https://twitter.com/dog_rates/status/821149554...                12   \n",
       "1872  https://twitter.com/dog_rates/status/675146535...                 7   \n",
       "\n",
       "      rating_denominator      name doggo floofer pupper puppo  \n",
       "2011                  10  Snickers  None    None   None  None  \n",
       "2239                  10      None  None    None   None  None  \n",
       "706                   10    Dallas  None    None   None  None  \n",
       "428                   10      Luca  None    None   None  None  \n",
       "1872                  10     Coops  None    None   None  None  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter_archive_enhanced.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2356 entries, 0 to 2355\n",
      "Data columns (total 17 columns):\n",
      "tweet_id                      2356 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2356 non-null object\n",
      "source                        2356 non-null object\n",
      "text                          2356 non-null object\n",
      "retweeted_status_id           181 non-null float64\n",
      "retweeted_status_user_id      181 non-null float64\n",
      "retweeted_status_timestamp    181 non-null object\n",
      "expanded_urls                 2297 non-null object\n",
      "rating_numerator              2356 non-null int64\n",
      "rating_denominator            2356 non-null int64\n",
      "name                          2356 non-null object\n",
      "doggo                         2356 non-null object\n",
      "floofer                       2356 non-null object\n",
      "pupper                        2356 non-null object\n",
      "puppo                         2356 non-null object\n",
      "dtypes: float64(4), int64(3), object(10)\n",
      "memory usage: 313.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_twitter_archive_enhanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19      RT @dog_rates: This is Canela. She attempted s...\n",
       "32      RT @Athletics: 12/10 #BATP https://t.co/WxwJmv...\n",
       "36      RT @dog_rates: This is Lilly. She just paralle...\n",
       "68      RT @dog_rates: This is Emmy. She was adopted t...\n",
       "73      RT @dog_rates: Meet Shadow. In an attempt to r...\n",
       "                              ...                        \n",
       "1023    RT @dog_rates: This is Shaggy. He knows exactl...\n",
       "1043    RT @dog_rates: Extremely intelligent dog here....\n",
       "1242    RT @twitter: @dog_rates Awesome Tweet! 12/10. ...\n",
       "2259    RT @dogratingrating: Exceptional talent. Origi...\n",
       "2260    RT @dogratingrating: Unoriginal idea. Blatant ...\n",
       "Name: text, Length: 181, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text for the retweeted tweets.\n",
    "df_twitter_archive_enhanced.query('retweeted_status_id.notnull() == True').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  13,   12,   14,    5,   17,   11,   10,  420,  666,    6,   15,\n",
       "         182,  960,    0,   75,    7,   84,    9,   24,    8,    1,   27,\n",
       "           3,    4,  165, 1776,  204,   50,   99,   80,   45,   60,   44,\n",
       "         143,  121,   20,   26,    2,  144,   88], dtype=int64),\n",
       " array([ 10,   0,  15,  70,   7,  11, 150, 170,  20,  50,  90,  80,  40,\n",
       "        130, 110,  16, 120,   2], dtype=int64))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique numerators and denominators.\n",
    "df_twitter_archive_enhanced.rating_numerator.unique(), df_twitter_archive_enhanced.rating_denominator.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  13,   12,   14,    5,   17,   11,   10,  420,  666,    6,   15,\n",
       "        182,    0,   75,    7,    9,    8,    1,   27,    3,    4, 1776,\n",
       "         26,    2], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numerator when the denominator = 10\n",
    "df_twitter_archive_enhanced.query('rating_denominator == 10').rating_numerator.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jax', 'Ted', 'Jim', 'Gus', 'Rey', 'a', 'Aja', 'Jed', 'Leo', 'Ken',\n",
       "       'Max', 'Ava', 'Eli', 'Ash', 'not', 'Mia', 'one', 'Ike', 'Mo', 'Bo',\n",
       "       'Tom', 'Alf', 'Sky', 'Tyr', 'Moe', 'Sam', 'Ito', 'Doc', 'mad',\n",
       "       'Jay', 'Mya', 'an', 'O', 'Al', 'Lou', 'my', 'Eve', 'Dex', 'Ace',\n",
       "       'Zoe', 'Blu', 'his', 'all', 'Sid', 'old', 'Ole', 'Bob', 'the',\n",
       "       'Obi', 'by', 'Evy', 'Tug', 'Jeb', 'Dot', 'Mac', 'Ed', 'Taz', 'Cal',\n",
       "       'JD', 'Pip', 'Amy', 'Gin', 'Edd', 'Ben', 'Dug', 'Jo', 'Ron', 'Stu'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#names with length smaller than 4 chars\n",
    "df_twitter_archive_enhanced.query('name.str.len() < 4').name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775    This is O'Malley. That is how he sleeps. Doesn...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text for dogs with name 'O'\n",
    "df_twitter_archive_enhanced.query('name == \"O\"').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724    This is by far the most coordinated series of ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text for dogs with name 'by'\n",
    "df_twitter_archive_enhanced.query('name == \"by\"').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum of number of dogs for each clasification\n",
    "t = (df_twitter_archive_enhanced.query('doggo != \"None\"').tweet_id.count(),\n",
    "df_twitter_archive_enhanced.query('floofer != \"None\"').floofer.count(),\n",
    "df_twitter_archive_enhanced.query('pupper != \"None\"').pupper.count(),\n",
    "df_twitter_archive_enhanced.query('puppo != \"None\"').puppo.count())\n",
    "sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 17)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of dogs with at least one clasification.\n",
    "df_twitter_archive_enhanced[df_twitter_archive_enhanced['doggo'].str.contains(\"doggo\") | \n",
    "                            df_twitter_archive_enhanced['floofer'].str.contains(\"floofer\") |\n",
    "                            df_twitter_archive_enhanced['pupper'].str.contains(\"pupper\") |\n",
    "                            df_twitter_archive_enhanced['puppo'].str.contains(\"puppo\")].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>855851453814013952</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>puppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>854010172552949760</td>\n",
       "      <td>doggo</td>\n",
       "      <td>floofer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>817777686764523521</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>808106460588765185</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>802265048156610565</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>801115127852503040</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>785639753186217984</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>781308096455073793</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>775898661951791106</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>770093767776997377</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>759793422261743616</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>751583847268179968</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>741067306818797568</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>733109485275860992</td>\n",
       "      <td>doggo</td>\n",
       "      <td>None</td>\n",
       "      <td>pupper</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  doggo  floofer  pupper  puppo\n",
       "191   855851453814013952  doggo     None    None  puppo\n",
       "200   854010172552949760  doggo  floofer    None   None\n",
       "460   817777686764523521  doggo     None  pupper   None\n",
       "531   808106460588765185  doggo     None  pupper   None\n",
       "565   802265048156610565  doggo     None  pupper   None\n",
       "575   801115127852503040  doggo     None  pupper   None\n",
       "705   785639753186217984  doggo     None  pupper   None\n",
       "733   781308096455073793  doggo     None  pupper   None\n",
       "778   775898661951791106  doggo     None  pupper   None\n",
       "822   770093767776997377  doggo     None  pupper   None\n",
       "889   759793422261743616  doggo     None  pupper   None\n",
       "956   751583847268179968  doggo     None  pupper   None\n",
       "1063  741067306818797568  doggo     None  pupper   None\n",
       "1113  733109485275860992  doggo     None  pupper   None"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets with two clasifications\n",
    "df_twitter_archive_enhanced[\n",
    "    df_twitter_archive_enhanced[[\"doggo\",\"floofer\",\"pupper\",\"puppo\"]].\n",
    "                            isin([\"doggo\",\"floofer\",\"pupper\",\"puppo\"]).sum(axis=1)> 1][['tweet_id',\"doggo\",\"floofer\",\"pupper\",\"puppo\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `df_image_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>772193107915964416</td>\n",
       "      <td>https://pbs.twimg.com/media/Crdhh_1XEAAHKHi.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Pembroke</td>\n",
       "      <td>0.367945</td>\n",
       "      <td>True</td>\n",
       "      <td>Chihuahua</td>\n",
       "      <td>0.223522</td>\n",
       "      <td>True</td>\n",
       "      <td>Pekinese</td>\n",
       "      <td>0.164871</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>710997087345876993</td>\n",
       "      <td>https://pbs.twimg.com/media/Cd34FClUMAAnvGP.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>malamute</td>\n",
       "      <td>0.281260</td>\n",
       "      <td>True</td>\n",
       "      <td>Eskimo_dog</td>\n",
       "      <td>0.232641</td>\n",
       "      <td>True</td>\n",
       "      <td>Pembroke</td>\n",
       "      <td>0.091602</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>745314880350101504</td>\n",
       "      <td>https://pbs.twimg.com/media/Clfj6RYWMAAFAOW.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>ice_bear</td>\n",
       "      <td>0.807762</td>\n",
       "      <td>False</td>\n",
       "      <td>great_white_shark</td>\n",
       "      <td>0.027040</td>\n",
       "      <td>False</td>\n",
       "      <td>fountain</td>\n",
       "      <td>0.022052</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>699036661657767936</td>\n",
       "      <td>https://pbs.twimg.com/media/CbN6IW4UYAAyVDA.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Chihuahua</td>\n",
       "      <td>0.222943</td>\n",
       "      <td>True</td>\n",
       "      <td>toyshop</td>\n",
       "      <td>0.179938</td>\n",
       "      <td>False</td>\n",
       "      <td>Weimaraner</td>\n",
       "      <td>0.163033</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>863907417377173506</td>\n",
       "      <td>https://pbs.twimg.com/media/C_03NPeUQAAgrMl.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>marmot</td>\n",
       "      <td>0.358828</td>\n",
       "      <td>False</td>\n",
       "      <td>meerkat</td>\n",
       "      <td>0.174703</td>\n",
       "      <td>False</td>\n",
       "      <td>weasel</td>\n",
       "      <td>0.123485</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                          jpg_url  \\\n",
       "1425  772193107915964416  https://pbs.twimg.com/media/Crdhh_1XEAAHKHi.jpg   \n",
       "1028  710997087345876993  https://pbs.twimg.com/media/Cd34FClUMAAnvGP.jpg   \n",
       "1227  745314880350101504  https://pbs.twimg.com/media/Clfj6RYWMAAFAOW.jpg   \n",
       "886   699036661657767936  https://pbs.twimg.com/media/CbN6IW4UYAAyVDA.jpg   \n",
       "1953  863907417377173506  https://pbs.twimg.com/media/C_03NPeUQAAgrMl.jpg   \n",
       "\n",
       "      img_num         p1   p1_conf  p1_dog                 p2   p2_conf  \\\n",
       "1425        1   Pembroke  0.367945    True          Chihuahua  0.223522   \n",
       "1028        1   malamute  0.281260    True         Eskimo_dog  0.232641   \n",
       "1227        2   ice_bear  0.807762   False  great_white_shark  0.027040   \n",
       "886         1  Chihuahua  0.222943    True            toyshop  0.179938   \n",
       "1953        1     marmot  0.358828   False            meerkat  0.174703   \n",
       "\n",
       "      p2_dog          p3   p3_conf  p3_dog  \n",
       "1425    True    Pekinese  0.164871    True  \n",
       "1028    True    Pembroke  0.091602    True  \n",
       "1227   False    fountain  0.022052   False  \n",
       "886    False  Weimaraner  0.163033    True  \n",
       "1953   False      weasel  0.123485   False  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image_predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2075 entries, 0 to 2074\n",
      "Data columns (total 12 columns):\n",
      "tweet_id    2075 non-null int64\n",
      "jpg_url     2075 non-null object\n",
      "img_num     2075 non-null int64\n",
      "p1          2075 non-null object\n",
      "p1_conf     2075 non-null float64\n",
      "p1_dog      2075 non-null bool\n",
      "p2          2075 non-null object\n",
      "p2_conf     2075 non-null float64\n",
      "p2_dog      2075 non-null bool\n",
      "p3          2075 non-null object\n",
      "p3_conf     2075 non-null float64\n",
      "p3_dog      2075 non-null bool\n",
      "dtypes: bool(3), float64(3), int64(2), object(4)\n",
      "memory usage: 152.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_image_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_num</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>666051853826850816</td>\n",
       "      <td>box_turtle</td>\n",
       "      <td>False</td>\n",
       "      <td>mud_turtle</td>\n",
       "      <td>False</td>\n",
       "      <td>terrapin</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>666104133288665088</td>\n",
       "      <td>hen</td>\n",
       "      <td>False</td>\n",
       "      <td>cock</td>\n",
       "      <td>False</td>\n",
       "      <td>partridge</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>666268910803644416</td>\n",
       "      <td>desktop_computer</td>\n",
       "      <td>False</td>\n",
       "      <td>desk</td>\n",
       "      <td>False</td>\n",
       "      <td>bookcase</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>666293911632134144</td>\n",
       "      <td>three-toed_sloth</td>\n",
       "      <td>False</td>\n",
       "      <td>otter</td>\n",
       "      <td>False</td>\n",
       "      <td>great_grey_owl</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>666362758909284353</td>\n",
       "      <td>guinea_pig</td>\n",
       "      <td>False</td>\n",
       "      <td>skunk</td>\n",
       "      <td>False</td>\n",
       "      <td>hamster</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>1</td>\n",
       "      <td>880935762899988482</td>\n",
       "      <td>street_sign</td>\n",
       "      <td>False</td>\n",
       "      <td>umbrella</td>\n",
       "      <td>False</td>\n",
       "      <td>traffic_light</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>1</td>\n",
       "      <td>881268444196462592</td>\n",
       "      <td>tusker</td>\n",
       "      <td>False</td>\n",
       "      <td>Indian_elephant</td>\n",
       "      <td>False</td>\n",
       "      <td>ibex</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>1</td>\n",
       "      <td>886680336477933568</td>\n",
       "      <td>convertible</td>\n",
       "      <td>False</td>\n",
       "      <td>sports_car</td>\n",
       "      <td>False</td>\n",
       "      <td>car_wheel</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>1</td>\n",
       "      <td>887517139158093824</td>\n",
       "      <td>limousine</td>\n",
       "      <td>False</td>\n",
       "      <td>tow_truck</td>\n",
       "      <td>False</td>\n",
       "      <td>shopping_cart</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>1</td>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>orange</td>\n",
       "      <td>False</td>\n",
       "      <td>bagel</td>\n",
       "      <td>False</td>\n",
       "      <td>banana</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      img_num            tweet_id                p1  p1_dog               p2  \\\n",
       "6           1  666051853826850816        box_turtle   False       mud_turtle   \n",
       "17          1  666104133288665088               hen   False             cock   \n",
       "18          1  666268910803644416  desktop_computer   False             desk   \n",
       "21          1  666293911632134144  three-toed_sloth   False            otter   \n",
       "25          1  666362758909284353        guinea_pig   False            skunk   \n",
       "...       ...                 ...               ...     ...              ...   \n",
       "2021        1  880935762899988482       street_sign   False         umbrella   \n",
       "2022        1  881268444196462592            tusker   False  Indian_elephant   \n",
       "2046        1  886680336477933568       convertible   False       sports_car   \n",
       "2052        1  887517139158093824         limousine   False        tow_truck   \n",
       "2074        1  892420643555336193            orange   False            bagel   \n",
       "\n",
       "      p2_dog              p3  p3_dog  \n",
       "6      False        terrapin   False  \n",
       "17     False       partridge   False  \n",
       "18     False        bookcase   False  \n",
       "21     False  great_grey_owl   False  \n",
       "25     False         hamster   False  \n",
       "...      ...             ...     ...  \n",
       "2021   False   traffic_light   False  \n",
       "2022   False            ibex   False  \n",
       "2046   False       car_wheel   False  \n",
       "2052   False   shopping_cart   False  \n",
       "2074   False          banana   False  \n",
       "\n",
       "[324 rows x 8 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets that have not been predicted as a dog in any of the three predictions.\n",
    "df_image_predictions.query('p1_dog == False & p2_dog == False & p3_dog == False')[['img_num',\n",
    "                                                                                   'tweet_id',\n",
    "                                                                                   'p1','p1_dog',\n",
    "                                                                                   'p2','p2_dog',\n",
    "                                                                                   'p3','p3_dog']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `df_tweepy_extractions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entities_name</th>\n",
       "      <th>entities_screen_name</th>\n",
       "      <th>entities_type</th>\n",
       "      <th>entities_user_id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorites_count_retweet</th>\n",
       "      <th>mentions_name</th>\n",
       "      <th>mentions_screen_name</th>\n",
       "      <th>mentions_user_id</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_rwetweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_count_retweet</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>[johnny yuen]</td>\n",
       "      <td>[johnny167167]</td>\n",
       "      <td>[user]</td>\n",
       "      <td>[393334099]</td>\n",
       "      <td>4876.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1361.0</td>\n",
       "      <td></td>\n",
       "      <td>702217446468493312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5411.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1584.0</td>\n",
       "      <td></td>\n",
       "      <td>712438159032893441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>14144.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3493.0</td>\n",
       "      <td></td>\n",
       "      <td>872820683541237760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>[Dan Morrow]</td>\n",
       "      <td>[DanielAMorrow]</td>\n",
       "      <td>[user]</td>\n",
       "      <td>[71047424]</td>\n",
       "      <td>2700.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>656.0</td>\n",
       "      <td></td>\n",
       "      <td>685321586178670592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1452.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>378.0</td>\n",
       "      <td></td>\n",
       "      <td>689999384604450816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>9775</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3986.0</td>\n",
       "      <td>3986</td>\n",
       "      <td>791780927877898241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>809.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>227.0</td>\n",
       "      <td></td>\n",
       "      <td>668274247790391296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1945.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1030.0</td>\n",
       "      <td></td>\n",
       "      <td>671544874165002241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>[Kelli Adrian]</td>\n",
       "      <td>[kadrian_15]</td>\n",
       "      <td>[user]</td>\n",
       "      <td>[2449586958]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17348</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4907.0</td>\n",
       "      <td>4907</td>\n",
       "      <td>816829038950027264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>[madison]</td>\n",
       "      <td>[_MaddyElizabeth]</td>\n",
       "      <td>[user]</td>\n",
       "      <td>[620042575]</td>\n",
       "      <td>3704.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1478.0</td>\n",
       "      <td></td>\n",
       "      <td>695314793360662529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       entities_name entities_screen_name entities_type entities_user_id  \\\n",
       "1350   [johnny yuen]       [johnny167167]        [user]      [393334099]   \n",
       "1214                                                                       \n",
       "98                                                                         \n",
       "1590    [Dan Morrow]      [DanielAMorrow]        [user]       [71047424]   \n",
       "1511                                                                       \n",
       "638                                                                        \n",
       "2203                                                                       \n",
       "2017                                                                       \n",
       "453   [Kelli Adrian]         [kadrian_15]        [user]     [2449586958]   \n",
       "1434       [madison]    [_MaddyElizabeth]        [user]      [620042575]   \n",
       "\n",
       "      favorite_count favorites_count_retweet  mentions_name  \\\n",
       "1350          4876.0                                          \n",
       "1214          5411.0                                          \n",
       "98           14144.0                                          \n",
       "1590          2700.0                                          \n",
       "1511          1452.0                                          \n",
       "638              0.0                    9775  [WeRateDogs®]   \n",
       "2203           809.0                                          \n",
       "2017          1945.0                                          \n",
       "453              0.0                   17348  [WeRateDogs®]   \n",
       "1434          3704.0                                          \n",
       "\n",
       "     mentions_screen_name mentions_user_id quoted_status_id  \\\n",
       "1350                                                          \n",
       "1214                                                          \n",
       "98                                                            \n",
       "1590                                                          \n",
       "1511                                                          \n",
       "638         [WeRateDogs®]     [4196983835]                    \n",
       "2203                                                          \n",
       "2017                                                          \n",
       "453         [WeRateDogs®]     [4196983835]                    \n",
       "1434                                                          \n",
       "\n",
       "     quoted_status_id_rwetweet  retweet_count retweet_count_retweet  \\\n",
       "1350                                   1361.0                         \n",
       "1214                                   1584.0                         \n",
       "98                                     3493.0                         \n",
       "1590                                    656.0                         \n",
       "1511                                    378.0                         \n",
       "638                                    3986.0                  3986   \n",
       "2203                                    227.0                         \n",
       "2017                                   1030.0                         \n",
       "453                                    4907.0                  4907   \n",
       "1434                                   1478.0                         \n",
       "\n",
       "                tweet_id  \n",
       "1350  702217446468493312  \n",
       "1214  712438159032893441  \n",
       "98    872820683541237760  \n",
       "1590  685321586178670592  \n",
       "1511  689999384604450816  \n",
       "638   791780927877898241  \n",
       "2203  668274247790391296  \n",
       "2017  671544874165002241  \n",
       "453   816829038950027264  \n",
       "1434  695314793360662529  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweepy_extractions.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2333 entries, 0 to 2332\n",
      "Data columns (total 14 columns):\n",
      "entities_name                2333 non-null object\n",
      "entities_screen_name         2333 non-null object\n",
      "entities_type                2333 non-null object\n",
      "entities_user_id             2333 non-null object\n",
      "favorite_count               2333 non-null float64\n",
      "favorites_count_retweet      2333 non-null object\n",
      "mentions_name                2333 non-null object\n",
      "mentions_screen_name         2333 non-null object\n",
      "mentions_user_id             2333 non-null object\n",
      "quoted_status_id             2333 non-null object\n",
      "quoted_status_id_rwetweet    2333 non-null object\n",
      "retweet_count                2333 non-null float64\n",
      "retweet_count_retweet        2333 non-null object\n",
      "tweet_id                     2333 non-null object\n",
      "dtypes: float64(2), object(12)\n",
      "memory usage: 255.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_tweepy_extractions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entities_name</th>\n",
       "      <th>entities_screen_name</th>\n",
       "      <th>entities_type</th>\n",
       "      <th>entities_user_id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorites_count_retweet</th>\n",
       "      <th>mentions_name</th>\n",
       "      <th>mentions_screen_name</th>\n",
       "      <th>mentions_user_id</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_rwetweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_count_retweet</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>1482</td>\n",
       "      <td>[Oakland A's]</td>\n",
       "      <td>[Oakland A's]</td>\n",
       "      <td>[19607400]</td>\n",
       "      <td>886053434075471873</td>\n",
       "      <td>886053434075471873</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101</td>\n",
       "      <td>886054160059072513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[Alexandra Gibson]</td>\n",
       "      <td>[Chappee_98]</td>\n",
       "      <td>[user]</td>\n",
       "      <td>[437072817]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68612</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>17287.0</td>\n",
       "      <td>17287</td>\n",
       "      <td>885311592912609280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>40266</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>6358.0</td>\n",
       "      <td>6358</td>\n",
       "      <td>879130579576475649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>7390</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1203.0</td>\n",
       "      <td>1203</td>\n",
       "      <td>878404777348136964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>20543</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>6237.0</td>\n",
       "      <td>6237</td>\n",
       "      <td>878316110768087041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2961</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1004.0</td>\n",
       "      <td>1004</td>\n",
       "      <td>746521445350707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>4490</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2129.0</td>\n",
       "      <td>2129</td>\n",
       "      <td>743835915802583040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>929</td>\n",
       "      <td>[Twitter, WeRateDogs®]</td>\n",
       "      <td>[Twitter, WeRateDogs®]</td>\n",
       "      <td>[783214, 4196983835]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>128.0</td>\n",
       "      <td>128</td>\n",
       "      <td>711998809858043904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>[WeRateDogs®]</td>\n",
       "      <td>[dog_rates]</td>\n",
       "      <td>[user]</td>\n",
       "      <td>[4196983835]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171</td>\n",
       "      <td>[We Rate Dog Ratings]</td>\n",
       "      <td>[We Rate Dog Ratings]</td>\n",
       "      <td>[4296831739]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>34.0</td>\n",
       "      <td>34</td>\n",
       "      <td>667550904950915073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>171</td>\n",
       "      <td>[We Rate Dog Ratings]</td>\n",
       "      <td>[We Rate Dog Ratings]</td>\n",
       "      <td>[4296831739]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>32.0</td>\n",
       "      <td>32</td>\n",
       "      <td>667550882905632768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           entities_name entities_screen_name entities_type entities_user_id  \\\n",
       "31                                                                             \n",
       "35    [Alexandra Gibson]         [Chappee_98]        [user]      [437072817]   \n",
       "67                                                                             \n",
       "72                                                                             \n",
       "73                                                                             \n",
       "...                  ...                  ...           ...              ...   \n",
       "1001                                                                           \n",
       "1021                                                                           \n",
       "1220                                                                           \n",
       "2236       [WeRateDogs®]          [dog_rates]        [user]     [4196983835]   \n",
       "2237                                                                           \n",
       "\n",
       "      favorite_count favorites_count_retweet           mentions_name  \\\n",
       "31               0.0                    1482           [Oakland A's]   \n",
       "35               0.0                   68612           [WeRateDogs®]   \n",
       "67               0.0                   40266           [WeRateDogs®]   \n",
       "72               0.0                    7390           [WeRateDogs®]   \n",
       "73               0.0                   20543           [WeRateDogs®]   \n",
       "...              ...                     ...                     ...   \n",
       "1001             0.0                    2961           [WeRateDogs®]   \n",
       "1021             0.0                    4490           [WeRateDogs®]   \n",
       "1220             0.0                     929  [Twitter, WeRateDogs®]   \n",
       "2236             0.0                     171   [We Rate Dog Ratings]   \n",
       "2237             0.0                     171   [We Rate Dog Ratings]   \n",
       "\n",
       "        mentions_screen_name      mentions_user_id    quoted_status_id  \\\n",
       "31             [Oakland A's]            [19607400]  886053434075471873   \n",
       "35             [WeRateDogs®]          [4196983835]                       \n",
       "67             [WeRateDogs®]          [4196983835]                       \n",
       "72             [WeRateDogs®]          [4196983835]                       \n",
       "73             [WeRateDogs®]          [4196983835]                       \n",
       "...                      ...                   ...                 ...   \n",
       "1001           [WeRateDogs®]          [4196983835]                       \n",
       "1021           [WeRateDogs®]          [4196983835]                       \n",
       "1220  [Twitter, WeRateDogs®]  [783214, 4196983835]                       \n",
       "2236   [We Rate Dog Ratings]          [4296831739]                       \n",
       "2237   [We Rate Dog Ratings]          [4296831739]                       \n",
       "\n",
       "     quoted_status_id_rwetweet  retweet_count retweet_count_retweet  \\\n",
       "31          886053434075471873          101.0                   101   \n",
       "35                                    17287.0                 17287   \n",
       "67                                     6358.0                  6358   \n",
       "72                                     1203.0                  1203   \n",
       "73                                     6237.0                  6237   \n",
       "...                        ...            ...                   ...   \n",
       "1001                                   1004.0                  1004   \n",
       "1021                                   2129.0                  2129   \n",
       "1220                                    128.0                   128   \n",
       "2236                                     34.0                    34   \n",
       "2237                                     32.0                    32   \n",
       "\n",
       "                tweet_id  \n",
       "31    886054160059072513  \n",
       "35    885311592912609280  \n",
       "67    879130579576475649  \n",
       "72    878404777348136964  \n",
       "73    878316110768087041  \n",
       "...                  ...  \n",
       "1001  746521445350707200  \n",
       "1021  743835915802583040  \n",
       "1220  711998809858043904  \n",
       "2236  667550904950915073  \n",
       "2237  667550882905632768  \n",
       "\n",
       "[165 rows x 14 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweepy_extractions.query('retweet_count_retweet != \"\"') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 14)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweepy_extractions.query('retweet_count_retweet != \"\"').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `df_scrapped_replies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>favs</th>\n",
       "      <th>full_name</th>\n",
       "      <th>image</th>\n",
       "      <th>language</th>\n",
       "      <th>references</th>\n",
       "      <th>replies</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>882762694511734784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mayhem Kevin™</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['4196983835']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>884544289853845504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cure pup</td>\n",
       "      <td>2017-07-11 00:46:07</td>\n",
       "      <td>837835939</td>\n",
       "      <td>@Mayham_Kevin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51053</th>\n",
       "      <td>724983749226668032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Katie McCarty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>und</td>\n",
       "      <td>['4196983835']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>725043998922952704</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>2016-04-26 21:28:54</td>\n",
       "      <td>2264927300</td>\n",
       "      <td>@katiemccarty_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10423</th>\n",
       "      <td>875021211251597312</td>\n",
       "      <td>2.0</td>\n",
       "      <td>shaunie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['517724675', '4196983835', '398156819']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875024484000047105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>top marks from me, did me a spook x</td>\n",
       "      <td>2017-06-14 18:17:48</td>\n",
       "      <td>2400085956</td>\n",
       "      <td>@peace0fshit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21417</th>\n",
       "      <td>831911600680497154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Oli Whites Bae</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['4196983835', '239715983']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>831916716447891456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes! Well done those who saved him im so happy !</td>\n",
       "      <td>2017-02-15 18:22:56</td>\n",
       "      <td>831587130312970240</td>\n",
       "      <td>@MillieJ6941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60320</th>\n",
       "      <td>687732144991551489</td>\n",
       "      <td>2.0</td>\n",
       "      <td>abby hyde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>und</td>\n",
       "      <td>['2863626632']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>687820335345995776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AW</td>\n",
       "      <td>2016-01-15 03:15:21</td>\n",
       "      <td>2740380257</td>\n",
       "      <td>@itsabbyhyde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64802</th>\n",
       "      <td>678021115718029313</td>\n",
       "      <td>2.0</td>\n",
       "      <td>cassidy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['370393849']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>678446914342289409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I love him</td>\n",
       "      <td>2015-12-20 06:28:43</td>\n",
       "      <td>323349940</td>\n",
       "      <td>@casbeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45109</th>\n",
       "      <td>756651752796094464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sarah Busic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['1005260233', '19946206', '4196983835']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>757293591634903041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amazing</td>\n",
       "      <td>2016-07-24 21:17:16</td>\n",
       "      <td>394930362</td>\n",
       "      <td>@Sarahbusic4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57059</th>\n",
       "      <td>698703483621523456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['35832263']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699385125977587712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@huyanaphour this is true</td>\n",
       "      <td>2016-02-16 01:09:42</td>\n",
       "      <td>2159234231</td>\n",
       "      <td>@madisongoebel13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35238</th>\n",
       "      <td>796116448414461957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Abielle c(__)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['379440123', '4196983835', '34148664']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>796118333431091200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Someone looks like they're totally over the el...</td>\n",
       "      <td>2016-11-08 23:32:56</td>\n",
       "      <td>122664841</td>\n",
       "      <td>@AbielleRose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25060</th>\n",
       "      <td>819015331746349057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Toad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>['4196983835']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>819007295166304256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>isn't she technically the second doggi</td>\n",
       "      <td>2017-01-11 03:25:30</td>\n",
       "      <td>1544737268</td>\n",
       "      <td>@TOADFROGTHING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             conversation  favs       full_name image language  \\\n",
       "6068   882762694511734784   0.0  Mayhem Kevin™    NaN       en   \n",
       "51053  724983749226668032   1.0   Katie McCarty   NaN      und   \n",
       "10423  875021211251597312   2.0         shaunie   NaN       en   \n",
       "21417  831911600680497154   0.0  Oli Whites Bae   NaN       en   \n",
       "60320  687732144991551489   2.0       abby hyde   NaN      und   \n",
       "64802  678021115718029313   2.0         cassidy   NaN       en   \n",
       "45109  756651752796094464   1.0    Sarah Busic    NaN       en   \n",
       "57059  698703483621523456   1.0            mads   NaN       en   \n",
       "35238  796116448414461957   0.0   Abielle c(__)   NaN       en   \n",
       "25060  819015331746349057   0.0            Toad   NaN       en   \n",
       "\n",
       "                                     references  replies            reply_id  \\\n",
       "6068                             ['4196983835']      0.0  884544289853845504   \n",
       "51053                            ['4196983835']      1.0  725043998922952704   \n",
       "10423  ['517724675', '4196983835', '398156819']      0.0  875024484000047105   \n",
       "21417               ['4196983835', '239715983']      0.0  831916716447891456   \n",
       "60320                            ['2863626632']      0.0  687820335345995776   \n",
       "64802                             ['370393849']      0.0  678446914342289409   \n",
       "45109  ['1005260233', '19946206', '4196983835']      0.0  757293591634903041   \n",
       "57059                              ['35832263']      0.0  699385125977587712   \n",
       "35238   ['379440123', '4196983835', '34148664']      1.0  796118333431091200   \n",
       "25060                            ['4196983835']      0.0  819007295166304256   \n",
       "\n",
       "       retweets                                               text  \\\n",
       "6068        0.0                                           Cure pup   \n",
       "51053       0.0                                                      \n",
       "10423       0.0                top marks from me, did me a spook x   \n",
       "21417       0.0   yes! Well done those who saved him im so happy !   \n",
       "60320       0.0                                                 AW   \n",
       "64802       0.0                                         I love him   \n",
       "45109       0.0                                            amazing   \n",
       "57059       0.0                          @huyanaphour this is true   \n",
       "35238       0.0  Someone looks like they're totally over the el...   \n",
       "25060       0.0             isn't she technically the second doggi   \n",
       "\n",
       "                 timestamp             user_id         user_name  \n",
       "6068   2017-07-11 00:46:07           837835939     @Mayham_Kevin  \n",
       "51053  2016-04-26 21:28:54          2264927300   @katiemccarty_9  \n",
       "10423  2017-06-14 18:17:48          2400085956      @peace0fshit  \n",
       "21417  2017-02-15 18:22:56  831587130312970240      @MillieJ6941  \n",
       "60320  2016-01-15 03:15:21          2740380257      @itsabbyhyde  \n",
       "64802  2015-12-20 06:28:43           323349940          @casbeal  \n",
       "45109  2016-07-24 21:17:16           394930362      @Sarahbusic4  \n",
       "57059  2016-02-16 01:09:42          2159234231  @madisongoebel13  \n",
       "35238  2016-11-08 23:32:56           122664841      @AbielleRose  \n",
       "25060  2017-01-11 03:25:30          1544737268    @TOADFROGTHING  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scrapped_replies.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72909 entries, 0 to 72908\n",
      "Data columns (total 13 columns):\n",
      "conversation    72909 non-null object\n",
      "favs            72909 non-null float64\n",
      "full_name       72456 non-null object\n",
      "image           3864 non-null object\n",
      "language        72909 non-null object\n",
      "references      72909 non-null object\n",
      "replies         72909 non-null float64\n",
      "reply_id        72909 non-null int64\n",
      "retweets        72909 non-null float64\n",
      "text            65534 non-null object\n",
      "timestamp       72909 non-null object\n",
      "user_id         72909 non-null int64\n",
      "user_name       72909 non-null object\n",
      "dtypes: float64(3), int64(2), object(8)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_scrapped_replies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "\n",
    "##### `df_twitter_archive_enhanced` table:\n",
    "\n",
    "- The type of the columns `tweet_id`, `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id` should be string.\n",
    "- `doggo`,`floofer`,`pupper` and `puppo` columns should be categorical.\n",
    "- We are not intereste in the first par of the text (RT @XXXX:) when the tweet is a retweet. I already have this information in other columns.\n",
    "- There are 23 rows with rating_denominator different to 10.\n",
    "- From those rows with a denominator equal to 10, some have a numerator not very realistic.\n",
    "- The name of the dogs `a`, `O`, `by`, `an`, `the`, `his`, `all`and `my` are incorrect. \n",
    "- There are not many dogs classified as doggo, floofer, etc. And 14 of them have double clasification.\n",
    "\n",
    "\n",
    "##### `df_image_predictions` table:\n",
    "\n",
    "- The type of the column `tweet_id` should be string.\n",
    "\n",
    "\n",
    "##### `df_tweepy_extractions` table:\n",
    "\n",
    "- The type of the columns `favorites_count_retweet` and `retweet_count_retweet` should be integer. Anyway the column `retweet_count_retweet` has no sense because it has the same value as `retweet_count`.\n",
    "- Nulls represented as void strings in `entities_name`,\t`entities_screen_name`, `entities_type`, `entities_user_id`, `favorites_count_retweet`,\t`entions_name`,\t`mentions_screen_name`,\t`mentions_user_id`,\t`quoted_status_id`,\t`quoted_status_id_rwetweet` and `retweet_count_retweet`.\n",
    "\n",
    "\n",
    "##### `df_scrapped_replies` table:\n",
    "\n",
    "- The type of the columns `favs`, `replies` and `retweets` should be integer instead of float.\n",
    "- The type of the columns `user_id` and `reply_id` should be a string.\n",
    "- `language` type should be categorical.\n",
    "\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wrangling] *",
   "language": "python",
   "name": "conda-env-wrangling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
